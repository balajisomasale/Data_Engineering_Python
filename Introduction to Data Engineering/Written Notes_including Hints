Contains all the hints and Explanations of theory Qns too.

DataCamp Data Engineering Introduction Codes:
----------------------------------------------
Tasks of the data engineer:
---------------------------
The video presented several tasks of the data engineer. You saw that there are some differences between the tasks of data scientists and the tasks of data engineers.

Below are three essential tasks that need to happen in a data-driven company. Can you find the one that best fits the job of a data engineer?

Answer the question
50 XP

Possible Answers
Apply a statistical model to a large dataset to find outliers.
press
1:Answer
Set up scheduled ingestion of data from the application databases to an analytical database.
press
2
Come up with a database schema for an application.
press
============================================================================================================================================================================

Data engineering problems:
--------------------------
For this exercise, imagine you work in a medium-scale company that hosts an online market for pet toys. As the company is growing, there are unmistakably some technical growing pains.

As the first data engineer, you observe some problems and have to decide where you're best suited to be of help.

Answer the question
50 XP
Possible Answers
Data scientists are querying the online store databases directly and slowing down the functioning of the application since it's using the same database.
press
1:Answer
Harmful product recommendations are affecting the sales numbers of the online store.
press
2
The online store is slow because the application's database server doesn't have enough memory.
press
3
============================================================================================================================================================================
Kinds of databases
In the video, you saw that databases are an essential tool for data engineers. The data engineer's workflow often begins and ends with databases.

However, databases are not always the first step. Sometimes, data has to be pulled in from external APIs or raw files. Can you identify the database in the schematics?

Instructions
50XP

Possible Answers
All database nodes are on the left.
All nodes on the left and the analytics node on the right are databases.
Accounting, Online Store, Product Catalog, and Analytics are databases.Answer

============================================================================================================================================================================

Processing tasks:
-----------------
Data engineers often have to join, clean, or organize data before loading it into a destination analytics database. This is done in the data processing, 
or data transformation step.

In the diagram, this is the intermediate step. Can you select the most correct statement about data processing?

Instructions
50XP
Possible Answers
Data processing is often done on a single, very powerful machine.
Data processing is distributed over clusters of virtual machines.Answer
Data processing is often very complicated because you have to manually distribute workload over several computers.

============================================================================================================================================================================

Scheduling tools:
-----------------
The last piece of the puzzle is the scheduler. In this example, Apache Airflow is used. You could see the scheduler as the glue of a data engineering system, 
holding each small piece together and organizing how they work together.

Do you know which one is not a responsibility of the scheduler?

Instructions
50XP
Possible Answers
Make sure jobs run in a specific order and all dependencies are resolved correctly.
Make sure the jobs run at midnight UTC each day.
Scale up the number of nodes when there's lots of data to be processed.Answer

============================================================================================================================================================================

Why cloud computing?

In the video you saw the benefits of using cloud computing as opposed to self-hosting data centers. Can you select the most correct statement about cloud computing?

Answer the question
50 XP
Possible Answers
Cloud computing is always cheaper.
press
1:Answer
The cloud can provide you with the resources you need, when you need them.
press
2
On premise machines give me full control over the situation when things break.
press
3
============================================================================================================================================================================

Topic 01:
---------
Joining on relations:

Database Schema for Customer and Order

customer:   

id
f_name
l_name

order:

id
customer_id
product_name
product_price

You've learned that you can use the read_sql() function from pandas to query the database. 
The real power of SQL is the ability to join information from multiple tables quickly. 
You do this by using the JOIN statement.

Note:When joining two or more tables, pandas puts all the columns of the query result into a DataFrame.

Complete the SELECT statement, so it joins the "Customer" with the "Order" table.
Print the id column of data. What do you see?

# Complete the SELECT statement
data = pd.read_sql("""
SELECT * FROM "Customer"
INNER JOIN "Order"
ON  "Order"."customer_id"="Customer"."id"
""", db_engine)

# Show the id column of data
print(data.id)
============================================================================================================================================================================

Topic:parallel computing
------------------------
Why parallel computing?

You've seen the benefits of parallel computing. However, you've also seen it's not the silver bullet to fix all problems related to computing.

1.Which of these statements is not correct?

Answer the question

a.Parallel computing can be used to speed up any task.
Answer:no we can't
b.Parallel computing can optimize the use of multiple processing units.

c.Parallel computing can optimize the use of memory between several machines.

d.Not entirely, frameworks like Spark optimize the use of memory between several machines.
============================================================================================================================================================================
Tpoic:From task to subtasks:

For this exercise, you will be using parallel computing to apply the function take_mean_age() that calculates the average athlete's age in a given year
in the Olympics events dataset.The DataFrame athlete_events has been loaded for you and contains amongst others, two columns:

Year: the year the Olympic event took place
Age: the age of the Olympian

You will be using the multiprocessor.Pool API which allows you to distribute your workload over several processes. 
The function parallel_apply() is defined in the sample code. It takes in as input the function being applied, the grouping used, 
and the number of cores needed for the analysis. Note that the @print_timing decorator is used to time each operation.

Instructions
100 XP
Complete the code, so you apply take_mean_age with 1 core first, then 2 and finally 4 cores.


# Function to apply a function over multiple cores
@print_timing
def parallel_apply(apply_func, groups, nb_cores):
    with Pool(nb_cores) as p:
        results = p.map(apply_func, groups)
    return pd.concat(results)

# Parallel apply using 1 core
parallel_apply(take_mean_age, athlete_events.groupby('Year'), 1)


# Parallel apply using 2 cores
parallel_apply(take_mean_age, athlete_events.groupby('Year'), 2)


# Parallel apply using 4 cores
parallel_apply(take_mean_age, athlete_events.groupby('Year'), 4)

Note:
-----
you saw how to split up a task and use the low-level python multiprocessing.
Pool API to do calculations on several processing units
============================================================================================================================================================================

Topic:Using a DataFrame
-----------------------
In the previous exercise, you saw how to split up a task and use the low-level python multiprocessing.Pool API to do calculations on several processing units.
It's essential to understand this on a lower level, but in reality, you'll never use this kind of APIs. A more convenient way to parallelize an apply over several 
groups is using the dask framework and its abstraction of the pandas DataFrame, 

for example.

The pandas DataFrame, athlete_events, is available in your workspace.

Instructions 1/2
50 XP
1

Create 4 partitions of the athletes_events DataFrame using dd.from_pandas().
If you forgot the parameters of dd.from_pandas(), check out the slides again, or type help(dd.from_pandas) in the console!

import dask.dataframe as dd

# Set the number of partitions
athlete_events_dask = dd.from_pandas(athlete_events, npartitions = 4)
-------------------------------------------------------------------------
instructions 2/2
50 XP
2

Print out the mean age for each Year. Remember dask uses lazy evaluation.

import dask.dataframe as dd

# Set the number of pratitions
athlete_events_dask = dd.from_pandas(athlete_events, npartitions = 4)

# Calculate the mean Age per Year
print(athlete_events_dask.groupby('Year').Age.mean().compute())

============================================================================================================================================================================

Topic: A PySpark groupby
------------------------
You've seen how to use the dask framework and its DataFrame abstraction to do some calculations. However, as you've seen in the video, 
in the big data world Spark is probably a more popular choice for data processing.

In this exercise, you'll use the PySpark package to handle a Spark DataFrame. The data is the same as in previous exercises: 
participants of Olympic events between 1896 and 2016.

The Spark Dataframe, athlete_events_spark is available in your workspace.

The methods you're going to use in this exercise are:
-----------------------------------------------------
.printSchema(): helps print the schema of a Spark DataFrame.

.groupBy(): grouping statement for an aggregation.

.mean(): take the mean over each group.

.show(): show the results.

Instructions
100 XP
Find out the type of athlete_events_spark.
Find out the schema of athlete_events_spark.
Print out the mean age of the Olympians, grouped by year. Notice that spark has not actually calculated anything yet. You can call this lazy evaluation.
Take the previous result, and call .show() on the result to calculate the mean age.

# Print the type of athlete_events_spark
print(type(athlete_events_spark))

# Print the schema of athlete_events_spark
print(athlete_events_spark.printSchema())

# Group by the Year, and find the mean Age
print(athlete_events_spark.groupBy('Year').mean('Age'))

# Group by the Year, and find the mean Age
print(athlete_events_spark.groupBy('Year').mean('Age').show())

============================================================================================================================================================================

Topic:Running PySpark files
---------------------------
In this exercise, you're going to run a PySpark file using spark-submit. This tool can help you submit your application to a spark cluster.

For the sake of this exercise, you're going to work with a local Spark instance running on 4 threads. 
The file you need to submit is in /home/repl/spark-script.py. Feel free to read the file:

cat /home/repl/spark-script.py
------------------------------
from pyspark.sql import SparkSession


if __name__ == "__main__":
    spark = SparkSession.builder.getOrCreate()
    athlete_events_spark = (spark
        .read
        .csv("/home/repl/datasets/athlete_events.csv",
             header=True,
             inferSchema=True,
             escape='"'))

    athlete_events_spark = (athlete_events_spark
        .withColumn("Height",
                    athlete_events_spark.Height.cast("integer")))

    print(athlete_events_spark
        .groupBy('Year')
        .mean('Height')
        .orderBy('Year')
        .show())
--------------------------------------------
You can use spark-submit as follows:

spark-submit \
  --master local[4] \
  /home/repl/spark-script.py
What does this output? Note that it may take a few seconds to get your results.

Possible Answers
An error.
A DataFrame with average Olympian heights by year.Answers
A DataFrame with Olympian ages.

============================================================================================================================================================================
Airflow, Luigi and cron:
------------------------
In the video, you saw several tools that can help you with the scheduling of your Spark jobs. You saw the limitations of cron and how it has led to the development 
of frameworks like Airflow and Luigi.

There's a lot of useful features in Airflow, but can you select the feature from the list below which is also provided by cron?

Answer the question
50 XP
Possible Answers
You can program your workflow in Python.
press
1
You can use a directed acyclic graph as a model for dependency resolving.
press
2
You have exact control over the time at which jobs run.Answer

============================================================================================================================================================================
Airflow DAGs:
-------------
Please refere for more:
https://airflow.readthedocs.io/en/latest/dag-run.html
------------------------------------------------------
In Airflow, a pipeline is represented as a Directed Acyclic Graph or DAG. The nodes of the graph represent tasks that are executed. 
The directed connections between nodes represent dependencies between the tasks.

Representing a data pipeline as a DAG makes much sense, as some tasks need to finish before others can start. You could compare this to an assembly line 
in a car factory. The tasks build up, and each task can depend on previous tasks being finished. A fictional DAG could look something like this:

Example DAG
Assembling the frame happens first, then the body and tires and finally you paint. Let's reproduce the example above in code.

Instructions 1/2
50 XP
1
2

First, the DAG needs to run on every hour at minute 0. Fill in the schedule_interval keyword argument using the crontab notation. 
For example, every hour at minute N would be N * * * *. Remember, you need to run at minute 0.

# Create the DAG object
dag = DAG(dag_id="car_factory_simulation",
          default_args={"owner": "airflow","start_date": airflow.utils.dates.days_ago(2)},
          schedule_interval="0 * * * *")
		  
The downstream flow should match what you can see in the image above. The first step has already been filled in for you.

# Create the DAG object
dag = DAG(dag_id="car_factory_simulation",
          default_args={"owner": "airflow","start_date": airflow.utils.dates.days_ago(2)},
          schedule_interval="0 * * * *")


# Task definitions
assemble_frame = BashOperator(task_id="assemble_frame", bash_command='echo "Assembling frame"', dag=dag)
place_tires = BashOperator(task_id="place_tires", bash_command='echo "Placing tires"', dag=dag)
assemble_body = BashOperator(task_id="assemble_body", bash_command='echo "Assembling body"', dag=dag)
apply_paint = BashOperator(task_id="apply_paint", bash_command='echo "Applying paint"', dag=dag)

# Complete the downstream flow
assemble_frame.set_downstream(place_tires)
assemble_frame.set_downstream(assemble_body)
assemble_body.set_downstream(apply_paint)

============================================================================================================================================================================
Data sources:
--------------
In the previous video you've learned about three ways of extracting data:

Extract from text files, like .txt or .csv
Extract from APIs of web services, like the Hackernews API
Extract from a database, like a SQL application database for customer data
We also briefly touched upon row-oriented databases and OLTP.

Can you select the statement about these topics which is not true?

Answer the question
50 XP
Possible Answers

OLTP means the system is optimized for transactions.
press
1
APIs mostly use raw text to transfer data.Answer

Row-oriented databases and OLTP go hand-in-hand.
press
2
============================================================================================================================================================================

Fetch from an API:
-----------------
In the last video, you've seen that you can extract data from an API by sending a request to the API and parsing the response which was in JSON format. 
In this exercise, you'll be doing the same by using the requests library to send a request to the Hacker News API.

Hacker News is a social news aggregation website, specifically for articles related to computer science or the tech world in general. 
Each post on the website has a JSON representation, which you'll see in the response of the request in the exercise.

Instructions
100 XP
Use the correct method of the requests module to get the Hacker News post's JSON object.
Print out the response, parsed as a JSON.
Assign the "score" key of the post to post_score.


import requests

# Fetch the Hackernews post
resp = requests.get("https://hacker-news.firebaseio.com/v0/item/16222426.json")

# Print the response parsed as JSON
print(resp.json())

# Assign the score of the test to post_score
post_score = resp.json()["score"]
print(post_score)
============================================================================================================================================================================

Read from a database
---------------------
In this exercise, you're going to extract data that resides inside tables of a local PostgreSQL database. The data you'll be using is the Pagila example database.
The database backs a fictional DVD store application, and educational resources often use it as an example database.

You'll be creating and using a function that extracts a database table into a pandas DataFrame object. The tables you'll be extracting are:

film: the films that are rented out in the DVD store.
customer: the customers that rented films at the DVD store.
In order to connect to the database, you'll have to use a PostgreSQL connection URI, which looks something like this:

postgresql://[user[:password]@][host][:port][/database]
Instructions
100 XP
Instructions
100 XP
Complete the extract_table_to_pandas() function definition to include the table name in the query.
Fill in the connection URI. The host is localhost and port is 5432. The username and password are repl and password, respectively. The database is pagila.
Complete the function calls of extract_table_to_pandas() to extract the film and customer tables.


# Function to extract table to a pandas DataFrame
def extract_table_to_pandas(tablename, db_engine):
    query = "SELECT * FROM {}".format(tablename)
    return pd.read_sql(query, db_engine)

# Connect to the database using the connection URI
connection_uri = "postgresql://repl:password@localhost:5432/pagila" 
db_engine = sqlalchemy.create_engine(connection_uri)

# Extract the film table into a pandas DataFrame
extract_table_to_pandas('film', db_engine)

# Extract the customer table into a pandas DataFrame
extract_table_to_pandas('customer', db_engine)
===========================================================================================================================================================================

Splitting the rental rate:
--------------------------

In the video exercise, you saw how to use pandas to split the email address column of the film table in order to extract the users' domain names. 
Suppose you would want to have a better understanding of the rates users pay for movies, so you decided to divide the rental_rate column into dollars and cents.

In this exercise, you will use the same techniques used in the video exercises to do just that! The film table has been loaded into the pandas DataFrame film_df. 
Remember, the goal is to split up the rental_rate column into dollars and cents.

Use the .astype() method to convert the rental_rate column into a column of string objects, and assign the results to rental_rate_str.
Split rental_rate_str on '.' and expand the results into columns. Assign the results to rental_rate_expanded.
Assign the newly created columns into films_df using the column names rental_rate_dollar and rental_rate_cents respectively.

# Get the rental rate column as a string
rental_rate_str = film_df.rental_rate.astype(str)

# Split up and expand the column
rental_rate_expanded = rental_rate_str.str.split(".", expand=True)

# Assign the columns to film_df
film_df = film_df.assign(
    rental_rate_dollar=rental_rate_expanded[0],
    rental_rate_cents=rental_rate_expanded[1],
)
============================================================================================================================================================================

Prepare for transformations:
---------------------------
As mentioned in the video, before you can do transformations using PySpark, you need to get the data into the Spark framework. You saw how to do this using PySpark. 
Can you choose the correct code?

(A)

spark.read.jdbc("jdbc:postgresql://repl:password@localhost:5432/pagila",
                "customer")
(B)

spark.read.jdbc("jdbc:postgresql://localhost:5432/pagila",
                "customer",
                {"user":"repl","password":"password"})
(C)

spark.read.jdbc("jdbc:postgresql://repl:password@localhost:5432/pagila/customer")

B:answer
============================================================================================================================================================================

Joining with ratings:
---------------------
In the video exercise, you saw how to use transformations in PySpark by joining the film and ratings tables to create a new column that stores the average rating 
per customer. In this exercise, you're going to create more synergies between the film and ratings tables by using the same techniques you learned in the video 
exercise to calculate the average rating for every film.

The PySpark DataFrame with films, film_df and the PySpark DataFrame with ratings, rating_df, are available in your workspace.

Instructions
100 XP
Instructions
100 XP

Take the mean rating per film_id, and assign the result to ratings_per_film_df.
Complete the .join() statement to join on the film_id column.
Show the first 5 results of the resulting DataFrame.

# Use groupBy and mean to aggregate the column
ratings_per_film_df = rating_df.groupBy('film_id').mean('rating')

# Join the tables using the film_id column
film_df_with_ratings = film_df.join(
    ratings_per_film_df,
    film_df.film_id==ratings_per_film_df.film_id
)

# Show the 5 first results
print(film_df_with_ratings.show(5))
============================================================================================================================================================================

OLAP or OLTP:
------------
You saw that there's a difference between OLAP and OLTP operations. A small recap:

OLAP: Online analytical processing
OLTP: Online transaction processing
It's essential to use the right database for the right job. There's a list of statements below. Can you find the most appropriate statement that is true?

Answer the question
50 XP
Possible Answers

Typically, analytical databases are column-oriented.
press
1
Massively parallel processing (MPP) databases are usually column-oriented.
press
2
Databases optimized for OLAP are usually not great at OLTP operations.
press
3
Analytical and application databases have different use cases and should be separated if possible.
press

4.no of above
5.all of above is answer

============================================================================================================================================================================

Writing to a file:
-----------------
In the video, you saw that files are often loaded into a MPP database like Redshift in order to make it available for analysis.

The typical workflow is to write the data into columnar data files. These data files are then uploaded to a storage system and from there, 
they can be copied into the data warehouse. In case of Amazon Redshift, the storage system would be S3, for example.

The first step is to write a file to the right format. For this exercises you'll choose the Apache Parquet file format.
There's a PySpark DataFrame called film_sdf and a pandas DataFrame called film_pdf in your workspace.

Instructions
100 XP
Instructions
100 XP
Write the pandas DataFrame film_pdf to a parquet file called "films_pdf.parquet".
Write the PySpark DataFrame film_sdf to a parquet file called "films_sdf.parquet".

# Write the pandas DataFrame to parquet
film_pdf.to_parquet("films_pdf.parquet")

# Write the PySpark DataFrame to parquet
film_sdf.write.parquet("films_sdf.parquet")
============================================================================================================================================================================
Load into Postgres:
------------------
In this exercise, you'll write out some data to a PostgreSQL data warehouse. That could be useful when you have a result of some transformations, 
and you want to use it in an application.

For example, the result of a transformation could have added a column with film recommendations, and you want to use them in your online store.

There's a pandas DataFrame called film_pdf in your workspace.
As a reminder, here's the structure of a connection URI for sqlalchemy:

postgresql://[user[:password]@][host][:port][/database]
Instructions
100 XP
Instructions
100 XP

Complete the connection URI for to create the database engine. The user and password are repl and password respectively. The host is localhost, and the port is 5432. This time, the database is dwh.
Finish the call so we use the "store" schema in the database. If the table exists, replace it completely.

# Finish the connection URI
connection_uri = "postgresql://repl:password@localhost:5432/dwh"
db_engine_dwh = sqlalchemy.create_engine(connection_uri)

# Transformation step, join with recommendations data
film_pdf_joined = film_pdf.join(recommendations)

# Finish the .to_sql() call to write to store.film
film_pdf_joined.to_sql("film", db_engine_dwh, schema="store", if_exists="replace")

# Run the query to fetch the data
pd.read_sql("SELECT film_id, recommended_film_ids FROM store.film", db_engine_dwh)

============================================================================================================================================================================

Defining a DAG:
--------------
In the previous exercises you applied the three steps in the ETL process:

Extract: Extract the film PostgreSQL table into pandas.
Transform: Split the rental_rate column of the film DataFrame.
Load: Load a the film DataFrame into a PostgreSQL data warehouse.

The functions extract_film_to_pandas(), transform_rental_rate() and load_dataframe_to_film() are defined in your workspace. In this exercise, 
you'll add an ETL task to an existing DAG. The DAG to extend and the task to wait for are defined in your workspace are defined as dag and wait_for_table respectively.

Complete the etl() function by making use of the functions defined in the exercise description.
Make sure etl_task uses the etl callable.
Set up the correct upstream dependency. Note that etl_task should wait for wait_for_table to be finished.
The sample code contains a sample run. This means the ETL pipeline runs when you run the code.

# Define the ETL function
def etl():
    film_df = extract_film_to_pandas()
    film_df = transform_rental_rate(film_df)
    load_dataframe_to_film(film_df)

# Define the ETL task using PythonOperator
etl_task = PythonOperator(task_id='etl_film',
                          python_callable=etl,
                          dag=dag)

# Set the upstream to wait_for_table and sample run etl()
etl_task.set_upstream(wait_for_table)
etl()

============================================================================================================================================================================
Setting up Airflow:
------------------
In this exercise, you'll learn how to add a DAG to Airflow. To the right, you have a terminal at your disposal. The workspace comes with Airflow pre-configured, 
but it's easy to install on your own.

You'll need to move the dag.py file containing the DAG you defined in the previous exercise to, the DAGs folder. Here are the steps to find it:

The airflow home directory is defined in the AIRFLOW_HOME environment variable. Type echo $AIRFLOW_HOME to find out.
In this directory, find the airflow.cfg file. Use head to read the file, and find the value of the dags_folder.
Now you can find the folder and move the dag.py file there: mv ./dag.py <dags_folder>.

Which files does the DAGs folder have after you moved the file?

Possible Answers
It has one DAG file: dag.py.
It has two DAG files: dag.py and dag_recommendations.py (Answer)
It has three DAG files: dag.py, you_wont_guess_this_dag.py, and super_secret_dag.py.

===========================================================================================================================================================================
Interpreting the DAG:
--------------------
Now that you've placed the DAG file in the correct place, it's time to check out the Airflow Web UI.

Can you find the scheduled interval of the sample DAG?

Instructions
50XP
Possible Answers
Daily at midnight.
Hourly at 0 minutes and 0 seconds.
It runs once at midnight.(Answer)

============================================================================================================================================================================
Exploring the schema:
--------------------
Have a look at the diagram of the database schema of datacamp_application:
Possible Answers
The user_id column.
press
1
There is no relationship.
press
2
The course_id column. (Answer)
press
3
The combination of user_id and course_id columns.
press
============================================================================================================================================================================
Querying the table:
------------------
Now that you have a grasp of what's happening in the datacamp_application database, let's go ahead and write up a query for that database.

The goal is to get a feeling for the data in this exercise. You'll get the rating data for three sample users and then use a predefined helper function, 
print_user_comparison(), to compare the sets of course ids these users rated.

Instructions
100 XP
Instructions
100 XP

Complete the connection URI. The database is called datacamp_application. The host is localhost with port 5432. The username is repl and password is password.
Select the ratings of users with id: 4387, 18163 and 8770.
Fill in print_user_comparison() with the three users you selected.

# Complete the connection URI
connection_uri = "postgresql://repl:password@localhost:5432/datacamp_application"
db_engine = sqlalchemy.create_engine(connection_uri)

# Get user with id 4387
user1 = pd.read_sql("SELECT * FROM rating where user_id=4387", db_engine)

# Get user with id 18163
user2 = pd.read_sql("SELECT * FROM rating where user_id=18163", db_engine)

# Get user with id 8770
user3 = pd.read_sql("SELECT * FROM rating where user_id=8770",db_engine)

# Use the helper function to compare the 3 users
print_user_comparison(user1, user2, user3)
=============================================================================================================================================================================
Average rating per course:
--------------------------
A great way to recommend courses is to recommend top-rated courses, as DataCamp students often like courses that are highly rated by their peers.

In this exercise, you'll complete a transformation function transform_avg_rating() that aggregates the rating data using the pandas DataFrame's .groupby() method.
The goal is to get a DataFrame with two columns, a course id and its average rating:

course_id	avg_rating
123	4.72
111	4.62
...	...
In this exercise, you'll complete this transformation function, and apply it on raw rating data extracted via the helper function extract_rating_data() 
which extracts course ratings from the rating table.

Instructions
100 XP
Instructions
100 XP
Complete the transform_avg_rating() function by grouping by the course_id column, and taking the mean of the rating column.
Use extract_rating_data() to extract raw ratings data. It takes in as argument the database engine db_engines.
Use transform_avg_rating() on the raw rating data you've extracted.

# Complete the transformation function
def transform_avg_rating(rating_data):
  # Group by course_id and extract average rating per course
  avg_rating = rating_data.groupby('course_id').rating.mean()
  # Return sorted average ratings per course
  sort_rating = avg_rating.sort_values(ascending=False).reset_index()
  return sort_rating

# Extract the rating data into a DataFrame    
rating_data = extract_rating_data(db_engines)

# Use transform_avg_rating on the extracted data and print results
avg_rating_data = transform_avg_rating(rating_data)
print(avg_rating_data) 

==========================================================================================================================================================================
Filter out corrupt data:
-----------------------
One recurrent step you can expect in the transformation phase would be to clean up some incomplete data. In this exercise, you're going to look at course data, 
which has the following format:

course_id	title	description	programming_language
1	        Some Course	...	r
You're going to inspect this DataFrame and make sure there are no missing values by using the pandas DataFrame's .isnull().sum() methods. 
You will find that the programming_language column has some missing values.

As such, you will complete the transform_fill_programming_language() function by using the .fillna() method to fill missing values.

Print the number of missing values in course_data.
Fill the missing values of the programming_language column with 'r'.
Print out the number of missing values per column once more, this time for transformed.

course_data = extract_course_data(db_engines)

# Print out the number of missing values per column
print(course_data.isnull().sum())

# The transformation should fill in the missing values
def transform_fill_programming_language(course_data):
    imputed = course_data.fillna({"programming_language": "r"})
    return imputed

transformed = transform_fill_programming_language(course_data)

# Print out the number of missing values per column of transformed
print(transformed)


===========================================================================================================================================================================
Using the recommender transformation:
------------------------------------
In the last few exercises, you calculated the average rating per course and cleaned up some course data. You will use this data to produce viable recommendations 
for DataCamp students.

As a reminder, here are the decision rules for producing recommendations:

Use technology a student has rated the most.
Exclude courses a student has already rated.
Find the three top-rated courses from eligible courses.
In order to produce the final recommendations, you will use the average course ratings, and the list of eligible recommendations per user, stored in avg_course_ratings 
and courses_to_recommend respectively. You will do this by completing the transform_recommendations() function which merges both DataFrames and finds the top 3 highest 
rated courses to recommend per user.

Complete the transform_recommendations() function by merging both DataFrames and sorting the results by rating.
Store recommendations per user in the recommendations object.


# Complete the transformation function
def transform_recommendations(avg_course_ratings, courses_to_recommend):
    # Merge both DataFrames
    merged = courses_to_recommend.merge(avg_course_ratings) 
    # Sort values by rating and group by user_id
    grouped = merged.sort_values("rating", ascending = False).groupby('user_id')
    # Produce the top 3 values and sort by user_id
    recommendations = grouped.head(3).sort_values("user_id").reset_index()
    final_recommendations = recommendations[["user_id", "course_id","rating"]]
    # Return final recommendations
    return final_recommendations

# Use the function with the predefined DataFrame objects
recommendations = transform_recommendations(avg_course_ratings, courses_to_recommend)


==========================================================================================================================================================================
The target table:
----------------
In the previous exercises, you've calculated a DataFrame called recommendations. It contains pairs of user_id's' and course_id's, with a rating that represents 
the average rating of this course. The assumption is the highest rated course, which is eligible for a user would be best to recommend.

It's time to put this table into a database so that it can be used by several products like a recommendation engine or an emailing system.

Since it's a pandas.DataFrame object, you can use the .to_sql() method. Of course, you'll have to connect to the database using the connection URI first. 
The recommendations table is available in your environment.

Instructions
100 XP
Instructions
100 XP

Fill in the connection URI for the Postgres database on host localhost with port 5432. You can connect with user repl and password password. The database name is dwh.
Complete the load_to_dwh() function. It should write to the "recommendations" table and replace if the table exists.

connection_uri = "postgresql://repl:password@localhost:5432/dwh"
db_engine = sqlalchemy.create_engine(connection_uri)

def load_to_dwh(recommendations):
    recommendations.to_sql("recommendations", db_engine_dwh, schema="store",if_exists="replace")
	
==========================================================================================================================================================================
Defining the DAG:
----------------
In the previous exercises, you've completed the extract, transform and load phases separately. Now all of this is put together in one neat etl() function that you can 
discover in the console.

The etl() function extracts raw course and ratings data from relevant databases, cleans corrupt data and fills in missing value, computes average rating per course and 
creates recommendations based on the decision rules for producing recommendations, and finally loads the recommendations into a database.

As you might remember from the video, etl() accepts a single argument: db_engines. You can pass this to the task using op_kwargs in the PythonOperator. You can pass it a 
dictionary that will be filled in as kwargs in the callable.

Instructions
100 XP
Instructions
100 XP
Complete the DAG definition, so it runs daily. Make sure to use the cron notation.
Complete the PythonOperator() by passing the correct arguments. Other than etl, db_engines is also available in

# Define the DAG so it runs on a daily basis
dag = DAG(dag_id="recommendations",
          schedule_interval="0 0 * * *")

# Make sure `etl()` is called in the operator. Pass the correct kwargs.
task_recommendations = PythonOperator(
    task_id="recommendations_task",
    python_callable=etl,
    op_kwargs={"db_engines": db_engines},
)
===========================================================================================================================================================================
Enable the DAG:
--------------
It's time to enable the DAG you just created, so it can start running on a daily schedule.

To the right you can find the Airflow interface. The DAG you created is called recommendations.

Can you find how to enable the DAG?

Instructions
50XP
Possible Answers
By switching the left-hand slide from Off to On. (ans)
It's already enabled!
By clicking the play icon on the right-hand side.

===========================================================================================================================================================================

Querying the recommendations:
----------------------------
In the previous exercises, you've learned how to calculate a table with course recommendations on a daily basis. Now that this recommendations table is in the data 
warehouse, you could also quickly join it with other tables in order to produce important features for DataCamp students such as customized marketing emails, 
intelligent recommendations for students and other features.

In this exercise, you will get a taste of how the newly created recommendations table could be utilized by creating a function recommendations_for_user() which 
automatically gets the top recommended courses based per user ID for a particular rating threshold.

Instructions
100 XP
Instructions
100 XP
Complete the query in the recommendations_for_user() function definition. It should join the courses table.
Complete the read_sql() function in recommendations_for_user(). The params argument is incomplete.
Observe the results of the recommendations_for_user() function you defined in the last statements.

def recommendations_for_user(user_id, threshold=4.5):
  # Join with the courses table
  query = """
  SELECT title, rating FROM recommendations
    INNER JOIN courses ON courses.course_id = recommendations.course_id
    WHERE user_id=%(user_id)s AND rating>%(threshold)s
    ORDER BY rating DESC
  """
  # Add the threshold parameter
  predictions_df = pd.read_sql(query, db_engine, params = {"user_id": user_id, 
                                                           "threshold": threshold})
  return predictions_df.title.values

# Try the function you created
print(recommendations_for_user(12, 4.65))

============================================================================================================================================================================

